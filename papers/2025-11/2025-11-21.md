# arXiv Papers - 2025-11-21

## Papers of Interest

### 1. Dataset Distillation for Pre-Trained Self-Supervised Vision Models

**Authors:** George Cazenavette, Antonio Torralba, Vincent Sitzmann  
**Published:** 2025-11-20  
**arXiv ID:** 2511.16674v1  
**Link:** https://arxiv.org/abs/2511.16674v1

**Abstract (excerpt):**  
The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly i...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---

### 2. EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards

**Authors:** Omkat Thawakar, Shravan Venkatraman, Ritesh Thawkar et al.  
**Published:** 2025-11-20  
**arXiv ID:** 2511.16672v1  
**Link:** https://arxiv.org/abs/2511.16672v1

**Abstract (excerpt):**  
Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM ...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---

### 3. Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation

**Authors:** Ziyu Guo, Renrui Zhang, Hongyu Li et al.  
**Published:** 2025-11-20  
**arXiv ID:** 2511.16671v1  
**Link:** https://arxiv.org/abs/2511.16671v1

**Abstract (excerpt):**  
Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---

### 4. Learning to Think Fast and Slow for Visual Language Models

**Authors:** Chenyu Lin, Cheng Chi, Jinlin Wu et al.  
**Published:** 2025-11-20  
**arXiv ID:** 2511.16670v1  
**Link:** https://arxiv.org/abs/2511.16670v1

**Abstract (excerpt):**  
When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical ...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---

### 5. Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO

**Authors:** Junhao Cheng, Liang Hou, Xin Tao et al.  
**Published:** 2025-11-20  
**arXiv ID:** 2511.16669v1  
**Link:** https://arxiv.org/abs/2511.16669v1

**Abstract (excerpt):**  
While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---


## Reading Progress
- Total papers tracked: 5
- Papers read this month: 0
- Implementation attempts: 0

## Tags
#machine-learning #deep-learning #research
