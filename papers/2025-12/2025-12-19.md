# arXiv Papers - 2025-12-19

## Papers of Interest

### 1. Next-Embedding Prediction Makes Strong Vision Learners

**Authors:** Sihan Xu, Ziqiao Ma, Wenhao Chai et al.  
**Published:** 2025-12-18  
**arXiv ID:** 2512.16922v1  
**Link:** https://arxiv.org/abs/2512.16922v1

**Abstract (excerpt):**  
Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. Thi...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---

### 2. Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification

**Authors:** Qihao Liu, Chengzhi Mao, Yaojie Liu et al.  
**Published:** 2025-12-18  
**arXiv ID:** 2512.16921v1  
**Link:** https://arxiv.org/abs/2512.16921v1

**Abstract (excerpt):**  
Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---

### 3. AdaTooler-V: Adaptive Tool-Use for Images and Videos

**Authors:** Chaoyang Wang, Kaituo Feng, Dongyang Chen et al.  
**Published:** 2025-12-18  
**arXiv ID:** 2512.16918v1  
**Link:** https://arxiv.org/abs/2512.16918v1

**Abstract (excerpt):**  
Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessar...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---

### 4. Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning

**Authors:** Qihao Liu, Luoxin Ye, Wufei Ma et al.  
**Published:** 2025-12-18  
**arXiv ID:** 2512.16917v1  
**Link:** https://arxiv.org/abs/2512.16917v1

**Abstract (excerpt):**  
Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy j...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---

### 5. Discovering gravitational waveform distortions from lensing: a deep dive into GW231123

**Authors:** Juno C. L. Chan, Jose MarÃ­a Ezquiaga, Rico K. L. Lo et al.  
**Published:** 2025-12-18  
**arXiv ID:** 2512.16916v1  
**Link:** https://arxiv.org/abs/2512.16916v1

**Abstract (excerpt):**  
Gravitational waves (GWs) are unique messengers as they travel through the Universe without alteration except for gravitational lensing. Their long wavelengths make them susceptible to diffraction by cosmic structures, providing an unprecedented opportunity to map dark matter substructures. Identify...

**My Notes:**
- Key contribution:
- Relevance to my work:
- Implementation ideas:

**Status:** ðŸ“š To Read

---


## Reading Progress
- Total papers tracked: 5
- Papers read this month: 0
- Implementation attempts: 0

## Tags
#machine-learning #deep-learning #research
